{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWLFl+V/Z/HZoAoft2n19j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaoLua/Opella./blob/Data_Logger/Data_Logger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UggqvYCp5Z99"
      },
      "outputs": [],
      "source": [
        "import tabula\n",
        "import jpype\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# --- 1. SETUP DIRECTORY AND LIST PDF FILES ---\n",
        "datalogger_dir = '/content/datalogger/'\n",
        "os.makedirs(datalogger_dir, exist_ok=True)\n",
        "pdf_files = [f for f in os.listdir(datalogger_dir) if f.endswith('.pdf')]\n",
        "\n",
        "all_processed_dataframes = []\n",
        "total_tor_data = []\n",
        "\n",
        "# --- GET USER INPUT FOR GLOBAL FILTERING ---\n",
        "print(\"\\nStep 1: Thời gian nhập nguyên liệu vào kho\")\n",
        "while True:\n",
        "    try:\n",
        "        # Get date and time from user in YYYY/MM/DD HH:MM format\n",
        "        input_date_str = input(\"Nhập ngày (YYYY/MM/DD): \")\n",
        "        input_time_str = input(\"Nhập giờ (HH:MM): \")\n",
        "\n",
        "        # Combine and convert to a datetime object\n",
        "        cutoff_str = f\"{input_date_str} {input_time_str}:00\"\n",
        "        global_cutoff_datetime = pd.to_datetime(cutoff_str, format='%Y/%m/%d %H:%M:%S')\n",
        "        break\n",
        "    except ValueError:\n",
        "        print(\"Sai format. Thử lại.\")\n",
        "\n",
        "# --- GET USER INPUT FOR TOR TEMPERATURE THRESHOLD ---\n",
        "print(\"\\nStep 2: Nhiệt độ giới hạn của nguyên liệu\")\n",
        "while True:\n",
        "    try:\n",
        "        tor_threshold_temp = float(input(\"Nhập nhiệt độ (°C): \"))\n",
        "        break\n",
        "    except ValueError:\n",
        "        print(\"Sai rồi. Thử lại.\")\n",
        "\n",
        "\n",
        "# --- 3. PROCESS EACH PDF FILE ---\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(datalogger_dir, pdf_file)\n",
        "    logger_id = pdf_file\n",
        "\n",
        "    print(f\"\\nĐang xử lý file: {logger_id}\")\n",
        "\n",
        "    # --- EXTRACT TABLES FROM PDF ---\n",
        "    try:\n",
        "        dfs = tabula.read_pdf(pdf_path, pages=\"2-5\", stream=True, multiple_tables=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi đọc file PDF {pdf_file}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # --- RESHAPE DATA FROM WIDE TO LONG FORMAT ---\n",
        "    all_reshaped_chunks = []\n",
        "    for page_df in dfs:\n",
        "        for i in range(0, page_df.shape[1], 3):\n",
        "            if i + 2 < page_df.shape[1]:\n",
        "                chunk = page_df.iloc[:, i:i+3].copy()\n",
        "                chunk.columns = ['raw_col1', 'raw_col2', 'raw_col3']\n",
        "                all_reshaped_chunks.append(chunk)\n",
        "    if not all_reshaped_chunks:\n",
        "        # print(f\"No data chunks found in {pdf_file}. Skipping.\")\n",
        "        continue\n",
        "    raw_long_df = pd.concat(all_reshaped_chunks, ignore_index=True)\n",
        "\n",
        "    # --- CLEAN THE RESHAPED DATA ---\n",
        "    all_records = []\n",
        "    pattern = re.compile(r'(\\d{2}/\\d{2}/\\d{2})?\\s+(\\d{2}:\\d{2}:\\d{2})\\s+([\\d\\.\\-]+)')\n",
        "\n",
        "    combined_series = raw_long_df['raw_col1'].fillna('') + ' ' + \\\n",
        "                      raw_long_df['raw_col2'].fillna('') + ' ' + \\\n",
        "                      raw_long_df['raw_col3'].fillna('')\n",
        "\n",
        "    for text in combined_series:\n",
        "        matches = pattern.findall(text)\n",
        "        for date_val, time_val, temp_val in matches:\n",
        "            all_records.append({\n",
        "                'date': date_val,\n",
        "                'time': time_val,\n",
        "                'temperature': temp_val})\n",
        "\n",
        "    if not all_records:\n",
        "        # print(f\"No records found in {pdf_file}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # --- CREATE THE DATAFRAME ---\n",
        "    datalogger = pd.DataFrame(all_records)\n",
        "    datalogger['datetime'] = pd.to_datetime(datalogger['date'] + ' ' + datalogger['time'], format='%m/%d/%y %H:%M:%S', errors='coerce')\n",
        "    datalogger['temperature'] = pd.to_numeric(datalogger['temperature'], errors='coerce')\n",
        "    datalogger['temperature'] = datalogger['temperature'].astype('float16')\n",
        "    datalogger.drop(columns=['date', 'time'], inplace=True)\n",
        "\n",
        "    datalogger.dropna(subset=['datetime', 'temperature'], inplace=True)\n",
        "    datalogger.sort_values('datetime', inplace=True)\n",
        "    datalogger.drop_duplicates(subset=['datetime', 'temperature'], keep='first', inplace=True)\n",
        "    datalogger = datalogger[['datetime', 'temperature']].reset_index(drop=True)\n",
        "\n",
        "    if datalogger.empty:\n",
        "        # print(f\"No valid data remaining in {pdf_file} after cleaning. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # --- FILTER THE DATAFRAME USING GLOBAL CUTOFF ---\n",
        "    # print(f\"\\nStep 4: Filtering data to remove records after {global_cutoff_datetime} for {logger_id}...\")\n",
        "\n",
        "    # Keep only the rows where the datetime is less than or equal to the user's input\n",
        "    datalogger_filtered = datalogger[datalogger['datetime'] <= global_cutoff_datetime].copy()\n",
        "\n",
        "    # print(f\"Filtering complete for {logger_id}. {len(datalogger_filtered)} records remaining.\")\n",
        "\n",
        "    if datalogger_filtered.empty:\n",
        "        # print(f\"No data remaining after filtering for {logger_id}. Skipping TOR analysis.\")\n",
        "        continue\n",
        "\n",
        "    # --- Calculate Min and Max Temperature and their datetimes for the filtered data ---\n",
        "    min_temp = datalogger_filtered['temperature'].min()\n",
        "    max_temp = datalogger_filtered['temperature'].max()\n",
        "    min_temp_datetime = datalogger_filtered[datalogger_filtered['temperature'] == min_temp]['datetime'].min()\n",
        "    max_temp_datetime = datalogger_filtered[datalogger_filtered['temperature'] == max_temp]['datetime'].min()\n",
        "\n",
        "\n",
        "    # --- TOR ANALYSIS ---\n",
        "    print(f\"\\nStep 5: Thực hiện TOR analysis cho Logger ID {logger_id} với giới hạn nhiệt độ {tor_threshold_temp}°C...\")\n",
        "    datalogger_filtered['is_tor'] = datalogger_filtered['temperature'] > tor_threshold_temp\n",
        "\n",
        "    tor_periods = []\n",
        "    start_time = None\n",
        "\n",
        "    for i in range(len(datalogger_filtered)):\n",
        "        is_current_tor = datalogger_filtered.loc[i, 'is_tor']\n",
        "        is_previous_tor = False if i == 0 else datalogger_filtered.loc[i-1, 'is_tor']\n",
        "        is_next_tor = False if i == len(datalogger_filtered) - 1 else datalogger_filtered.loc[i+1, 'is_tor']\n",
        "\n",
        "        if is_current_tor and not is_previous_tor:\n",
        "            start_time = datalogger_filtered.loc[i, 'datetime']\n",
        "\n",
        "        if is_current_tor and not is_next_tor and start_time is not None:\n",
        "            end_time = datalogger_filtered.loc[i, 'datetime']\n",
        "            tor_periods.append({'TOR start': start_time, 'TOR stop': end_time})\n",
        "            start_time = None\n",
        "\n",
        "    tor_df = pd.DataFrame(tor_periods)\n",
        "\n",
        "    total_tor_file = pd.Timedelta(seconds=0)\n",
        "    if not tor_df.empty:\n",
        "        tor_df['Excursion duration'] = tor_df['TOR stop'] - tor_df['TOR start']\n",
        "\n",
        "        # Format Excursion duration\n",
        "        tor_df['Excursion duration'] = tor_df['Excursion duration'].apply(lambda x: f'{x.days*24 + x.seconds//3600:02d}:{x.seconds%3600//60:02d}')\n",
        "\n",
        "        # Add temperature at TOR start and stop\n",
        "        tor_df = pd.merge(tor_df, datalogger_filtered[['datetime', 'temperature']],\n",
        "                                left_on='TOR start', right_on='datetime', how='left')\n",
        "        tor_df.rename(columns={'temperature': 'temperature at tor start'}, inplace=True)\n",
        "        tor_df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "        tor_df = pd.merge(tor_df, datalogger_filtered[['datetime', 'temperature']],\n",
        "                                left_on='TOR stop', right_on='datetime', how='left')\n",
        "        tor_df.rename(columns={'temperature': 'temperature at tor stop'}, inplace=True)\n",
        "        tor_df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "        # Rearrange columns\n",
        "        final_tor_df = tor_df[['TOR start', 'temperature at tor start', 'TOR stop', 'temperature at tor stop', 'Excursion duration']]\n",
        "\n",
        "        # Add Logger ID\n",
        "        final_tor_df['Logger ID'] = logger_id\n",
        "\n",
        "        all_processed_dataframes.append(final_tor_df)\n",
        "\n",
        "        # Calculate total TOR for this file (in timedelta)\n",
        "        tor_df['Excursion duration_timedelta'] = tor_df['Excursion duration'].apply(lambda x: pd.to_timedelta(x + ':00'))\n",
        "        total_tor_file = tor_df['Excursion duration_timedelta'].sum()\n",
        "\n",
        "\n",
        "    else:\n",
        "        # print(f\"No TOR periods found in {pdf_file} after filtering.\")\n",
        "\n",
        "    # Store total TOR, min temp, max temp, and their datetimes for this logger\n",
        "    total_tor_data.append({\n",
        "        'Logger ID': logger_id,\n",
        "        'Total TOR': total_tor_file,\n",
        "        'Min Temperature': min_temp,\n",
        "        'Min Temp Datetime': min_temp_datetime,\n",
        "        'Max Temperature': max_temp,\n",
        "        'Max Temp Datetime': max_temp_datetime\n",
        "    })\n",
        "\n",
        "\n",
        "# --- 4. COMBINE DATAFRAMES ---\n",
        "if all_processed_dataframes:\n",
        "    combined_datalogger_df = pd.concat(all_processed_dataframes, ignore_index=True)\n",
        "    combined_datalogger_df = combined_datalogger_df.reset_index(drop=True)\n",
        "\n",
        "    # --- 5. CALCULATE AND FORMAT TOTAL TOR PER LOGGER ---\n",
        "    total_tor_summary_df = pd.DataFrame(total_tor_data)\n",
        "\n",
        "    def format_timedelta_as_ddhhmm(td):\n",
        "        \"\"\"Formats a timedelta object into 'dd:hh:mm' string.\"\"\"\n",
        "        total_seconds = td.total_seconds()\n",
        "        days = int(total_seconds // (24 * 3600))\n",
        "        hours = int((total_seconds % (24 * 3600)) // 3600)\n",
        "        minutes = int((total_seconds % 3600) // 60)\n",
        "        return f\"{days:02d}:{hours:02d}:{minutes:02d}\"\n",
        "\n",
        "    # Format the total TOR\n",
        "    total_tor_summary_df['Total TOR'] = total_tor_summary_df['Total TOR'].apply(format_timedelta_as_ddhhmm)\n",
        "\n",
        "\n",
        "    # --- 6. FINALIZE AND DISPLAY RESULTS ---\n",
        "    # Add a sequential number column for each Logger ID in the combined_datalogger_df\n",
        "    combined_datalogger_df['Number'] = combined_datalogger_df.groupby('Logger ID').cumcount() + 1\n",
        "\n",
        "    # Format temperature columns to 2 decimal places in the combined_datalogger_df\n",
        "    combined_datalogger_df['temperature at tor start'] = combined_datalogger_df['temperature at tor start'].round(2)\n",
        "    combined_datalogger_df['temperature at tor stop'] = combined_datalogger_df['temperature at tor stop'].round(2)\n",
        "\n",
        "    # Format min/max temperature columns to 2 decimal places in the summary df\n",
        "    total_tor_summary_df['Min Temperature'] = total_tor_summary_df['Min Temperature'].round(2)\n",
        "    total_tor_summary_df['Max Temperature'] = total_tor_summary_df['Max Temperature'].round(2)\n",
        "\n",
        "\n",
        "    # Rearrange columns to have 'Logger ID' first, then 'Number'\n",
        "    final_combined_df = combined_datalogger_df[['Logger ID', 'Number', 'TOR start', 'temperature at tor start', 'TOR stop', 'temperature at tor stop', 'Excursion duration']]\n",
        "\n",
        "    # print(\"--- Combined TOR Excursion Details with Logger ID, Number, and Temperatures ---\")\n",
        "    display(final_combined_df)\n",
        "\n",
        "    print(\"\\n--- Total Time Out of Refrigerator (TOR) per Logger ID, Min/Max Temperature, and Datetime ---\")\n",
        "    # Rearrange columns for summary display\n",
        "    total_tor_summary_df = total_tor_summary_df[['Logger ID', 'Total TOR', 'Min Temperature', 'Min Temp Datetime', 'Max Temperature', 'Max Temp Datetime']]\n",
        "    display(total_tor_summary_df)\n",
        "\n",
        "    # --- 7. EXPORT TO EXCEL ---\n",
        "    output_excel_path = \"/content/datalogger/tor_analysis_results.xlsx\"\n",
        "    with pd.ExcelWriter(output_excel_path) as writer:\n",
        "        final_combined_df.to_excel(writer, sheet_name='Excursion Details', index=False)\n",
        "        total_tor_summary_df.to_excel(writer, sheet_name='Total TOR Summary', index=False)\n",
        "\n",
        "    print(f\"\\nResults exported successfully to '{output_excel_path}'\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"No data processed from any PDF files.\")"
      ]
    }
  ]
}